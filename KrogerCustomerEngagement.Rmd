---
title: "Marketing's Effect on Customer Engagement"
author: "Beth Hilbert"
date: "`r Sys.Date()`"
output:
  html_document
---

#{.tabset .tabset-fade}

## 1 Introduction

### Business Question
My goal is investigate how promotion affects future customer engagement. Customer engagement will be measured by subsequent purchases of the product. This can quantify the impact of incremental sales by exposure to promotions. 

### Plan
I will use the Carbo_Loading dataset provided by 84.51. 


Two important fields I will use are 'upc' and 'feature_desc'. The  feature description indiates how the product was featured in marketing during a specific week, with options such as front page, back of wrap, etc. 


If the product was purchased during a feature, I will look at whether it was previously purchased and whether it was subsequently purchased by that household. 

### Benefit
The benefit of the analysis will be to see whether certain combinations of products and featured marketing are more effective in engaging customers long term. 

## 2 Packages

### Packages

The following packages are required to reproduce these results: 

```{r load_packages, message = FALSE, warning = FALSE}
# Load Required Packages
library(data.table) ## Used to set names without rewriting table
library(dendextend) ## for use drawaing dendrogram with clustering models
library(DT) ## Create functional tables in HTML
library(haven) ## Read statistical software data (original data is stored in SAS files)
library(tidyverse) ## Collection of R data science packages 
library(scales) ## formats axis labels with dollars and commas
```

Loading Tidyverse loads a collection of data science packages. From Tidyverse I am using: 

* dplyr to manipulate data  and join tables
* ggplot2 to visualize 
* purrr for functional programming 
* stringr to work with strings
* tidyr for tidying data. 

## 3 Data Preperation {.tabset .tabset-fade}

### 3.1 Load Data

**The first prep step is to understand the source of the data.**  I will use the Carbo_Loading dataset provided by 84.51. The [data](http://uc-r.github.io/data_wrangling/final-project) is a subset of four potentially related product commodities (pasta/sauce, pancake mix/syrup), capturing their sales and weekly promotions. Promotions are identified by location in weekly mailer, location of in-store display, and coupons redeemed. There are 927 products with their associated promotion campaigns over a 2 year period. The data includes 5,197,681 sale transactions with a grain of one row per household per UPC per day. This data set was prepared for classroom projects and case studies to allow students to interact with real-world data. 


Because the data is stored in a SAS file format (.sas7bdat), there are a few unique things to consider:

* The data is loaded using the read_sas command from the Haven package. 

* Some variables included attributes which allow them to later be read back into SAS. However these attributes were only included on some of the key fields, which caused errors when joining the tables together. For example the 'upc' variable had attributes in the transactions and weekly tables, but not in product table. I removed the attributes with zap_formats(). 

* SAS encodes nulls as blanks, so nulls are not identifiable with the typical is.na() function and do not show on summary() as missing values. The only nulls in the original datasets (encoded as blank strings) were in the product_size variable. However more nulls were introducted during the join process.


```{r load_data, message = FALSE, warning = FALSE }
# load data
files <- c('causal_lookup','product_lookup', 'store_lookup', 'transactions')
df_names <- c('weekly', 'product', 'store', 'transactions')

for (i in seq_along(files)) {
  #create file path
  full_path <- paste0("data/",files[i], '.sas7bdat')

  #import data
  if (file.exists(full_path)) {
   df <- read_sas(full_path)
   assign(df_names[i],df)
   df_names[i] <- zap_formats(df_names[i]) 
   rm(df)
  } else {
    response <- paste("There is no file with the name", full_path)
    print(response)
  }
}
```


### 3.2 Initial Data Structure

**The second prep step is to understand the initial structure of the data.**  The primary table is transactions and there are 3 lookup tables for products, stores, and weekly promotions.

* **Transaction Table** contains a sample of 2 years of carbo related transactions at the household level. The grain of the table is one row per household per UPC per day. There are `r dim(transactions)[1]` transaction rows and `r dim(transactions)[2]` variables. The variables are named: `r names(transactions)`.

* **Product Table** provides detailed product information for each UPC. There are `r dim(product)[1]` product rows and `r dim(product)[2]` variables. The variables are named: `r names(product)`. 

* **Weekly Table** provides weekly promotion activity for each UPC. In the weekly table, there are `r dim(weekly)[1]` UPC weekly promotion rows and `r dim(weekly)[2]` variables. The variables are named: `r names(weekly)`. 

* **Store Table** records each store's zip code. There are `r dim(store)[1]` store rows and `r dim(store)[2]` variables. The variables are named: `r names(store)`. 


**Comments on initial data structure:**

* Households are uniquely identified.

* Time is recorded by 'time_of_transaction'' expressed as 24 hour clock, 'week' expressed chronologically from 1 to 104, and 'day' expressed from 1 to 728. There is not an actual calendar date, only a relative chronological order of transactions.

* Geography is divided into two large regions encompassing portions of several states.

* Coupon indicates whether a coupon was used with the UPC on that day for that household.  

* Commodity refers to the 4 categories (Pasta, Pasta Sauce, Pancake Mix, Syrup). 

* Feature_desc describes the location of product on a weekly mailer.

* Display_desc describes location of in-store display containing the product.


```{r glimpse_data, message = FALSE, warning = FALSE}
for (i in seq_along(df_names)) {
  print(df_names[i])
  glimpse(get(df_names[i]))
}
```


### 3.3 Tidy the Data

**The third prep step is to look at the data.** 

Decisions based on data are dependent on data quality. A large part of time is spent preparing the data for analysis. We have to decide which data to gather and what features of the data are important. This step also considers what data is missing and how to create new variables and summaries to reflect these gaps. 

To begin with, I verified the data was stored in a 'tidy' format in which observations are recorded in rows (measure same unit across all attributes with one measurement per row), variables are columns (measure same attribute across observations), and there is one type of observational unit per table. 

I then looked at the values and distributions of each variable using summary(df) and unique(sort(df$var)). I was looking for anything unusual such as variations on spelling, capitalization, missing, or unexpected values. As a result, I made a few changes to the transactions, products and weekly tables. No changes were made to the store table.


**Tidying transaction data**

I looked at the general distribution of the transaction data. There are `r n_distinct(transactions$household) ` unique households. There is an average of `r round((n_distinct(transactions$basket) / n_distinct(transactions$household)),2) ` baskets per household over the 2 year period. This seems like a very small number of grocery trips. This is indicative to me that when the data was sampled, the focus was not on preserving transactions for an entire household. Because of this, I determined the data would not support analyzing a specific customers response to a campaign. So I changed the grain of my review from looking at transactions by individual households to looking at products.  

Coupons were used on products in `r round(mean(transactions$coupon)*100,2) ` % of the transactions. I do not know the industry standard, but it seems reasonable.

I then evaluated whether the dollar sales were reasonable. The sales ranged from $`r range(transactions$dollar_sales)[1] ` to $`r range(transactions$dollar_sales)[2] ` with a median sale of $`r median(transactions$dollar_sales)`.


```{r trans_dollars, results= FALSE, message = FALSE, warning = FALSE, errors = FALSE}
# Look closer at transaction$dollar_sales
negative_dollars <- transactions[transactions$dollar_sales < 0,]
count_negative_dollars <- count(negative_dollars)
percent_negative_dollars <- round((count(negative_dollars) / count(transactions))*100, 4)

zero_dollars <- transactions[transactions$dollar_sales == 0,]
count_zero_dollars <- count(zero_dollars)
percent_zero_dollars <- round((count(zero_dollars) / count(transactions))*100, 4)

# Evaluate how prevalent negative sales are
negative_dollars %>%
  group_by(upc) %>%
  tally() %>%
  arrange(desc(n))
```

* There were `r count_negative_dollars` sales that were negative which represents `r percent_negative_dollars`% of total transactions. I assumed negatives were appropriate as there could be returns. I initially considered removing these rows with negative sales from the data since returning an item would be unaffected by advertising or product placement in the store. But I decided to keep this data to see if some products were frequently returned.

* There were `r count_zero_dollars` sales that were $0 which represents `r percent_zero_dollars`% of total transactions. I kept these rows since sales of $0 could be the result of a promotion.

* There were some dollar values as high as $`r max(transactions$dollar_sales) ` and units as high as `r max(transactions$units) `. I used a boxplot to compare dollar sales to units and determined they were most likely within an expected range (more units purchased resulted in more dollars spent).

```{r message = FALSE, warning = FALSE, errors = FALSE}
#clean environment
remove_objects <- c('negative_dollars', 'count_negative_dollars', 'percent_negative_dollars', 
                  'zero_dollars', 'count_zero_dollars', 'percent_zero_dollars' )
rm(list = ls()[ls() %in% remove_objects])
```



```{r boxplot_unusual_transactions, message = FALSE, warning = FALSE }
# Plot potentially unusual transactions
boxplot(transactions$dollar_sales ~ transactions$units, 
        main = 'Potentially Unusual Transactions', xlab='Quantity', ylab = "Dollars")
```

There were a few changes to transaction dataframe:

* time_of_transactions - initially this was chr type, I coerced to integer type so they could be easily compared. I did not use a standard date format because other time related information is recorded chronologically, ignoring calendar date.

* week is in both transaction table and weekly table. The transaction table weeks range from 1 to 104, but the weekly table starts at week 43. I decided to keep al the transaction data instead of just the overlapping data, and my reasons will be discussed in the join section. 


```{r tidy_transactions, message = FALSE, warning = FALSE}
# Coerce time and coupon
transactions <- mutate(transactions, 
       time_of_transaction = as.numeric(time_of_transaction))
```



**Tidying weekly promotion data**

In the weekly promotion data there are `r length(unique(weekly$feature_desc))` types of features and `r length(unique(weekly$display_desc))` types of displays. I coerced both of these to factors. 

* Feature_desc represent ad promotions. I created factor levels with the assumption that items on the advertising wrap are more important than covers, which are more important than not featured items. 

* Display_desc represents in-store promotion location. For display I created factor levels with the assumption that End Caps are better than aisles, and front of store is better than rear. I will later use these levels as variable names, so I changed hyphens and slashes to underscores

```{r tidy_weekly, message = FALSE, warning = FALSE} 

#change hyphen and slashes to underscore 
weekly$display_desc <- str_replace_all(weekly$display_desc,"-","_")
weekly$display_desc <- str_replace_all(weekly$display_desc,"Promo/Seasonal Aisle","Promo_Seasonal Aisle")

# Factor feature and display descriptions
feature_levels = c('Wrap Front Feature', 
                  'Wrap Back Feature', 
                  'Wrap Interior Feature',
                  'Front Page Feature', 
                  'Back Page Feature',
                  'Interior Page Feature', 
                  'Interior Page Line Item',
                  'Not on Feature',
                  'Unknown',
                  'Pre_Collection')

display_levels = c('Store Front',
                  'Front End Cap',
                  'Side_Aisle End Cap',
                  'Promo_Seasonal Aisle',
                  'Secondary Location Display',
                  'Rear End Cap',
                  'Store Rear',
                  'Mid_Aisle End Cap',
                  'In_Aisle',
                  'In_Shelf',
                  'Not on Display',
                  'Unknown',
                  'Pre_Collection')

weekly <- mutate(weekly, 
                  feature_desc = factor(feature_desc, levels = feature_levels),
                  display_desc = factor(display_desc, levels = display_levels))

``` 


**Tidying product data**

There were a lot of changes needed in the product data.

I renamed and factored the four commodities.

```{r tidy_commodity, message = FALSE, warning = FALSE}
# Rename and factor commodities
product <- product %>%
  mutate(commodity = case_when(
                                str_detect(commodity, 'pasta sauce') ~ 'SAUCE',
                                str_detect(commodity, 'pasta') ~ 'PASTA',
                                str_detect(commodity, 'pancake mixes') ~ 'PANCAKE',
                                str_detect(commodity, 'syrups') ~ 'SYRUP',
                                TRUE ~ NA_character_))

commodity_level = c('PASTA', 
                  'SAUCE', 
                  'PANCAKE',
                  'SYRUP')

product <- mutate(product, 
                  commodity = factor(commodity, levels = commodity_level))

```

I created a variable named 'product_attribute' which identifies a characteristic to group by (such as buttermilk, maple, fettucini). This also cleaned inconsistent product names so that similar products can be compared. 

```{r create_product_attribute, message = FALSE, warning = FALSE}
# Create variable 'product_attribute' for grouping and renaming descriptions consistently
product <- product %>%
  mutate(product_attribute = case_when(
                                str_detect(product_description, 'BUTTERM') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTMK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTRMK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTRMLK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'MILK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'MLK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'LITE') ~ 'LITE',
                                str_detect(product_description, 'BELG') ~ 'BELGIAN',
                                str_detect(product_description, 'BUCKWHEAT') ~ 'BUCKWHEAT',
                                str_detect(product_description, 'BLUEB') ~ 'BERRY',
                                str_detect(product_description, 'BERRY') ~ 'BERRY',
                                str_detect(product_description, 'BLUBRY') ~ 'BERRY',
                                str_detect(product_description, 'MAPLE') ~ 'MAPLE',
                                str_detect(product_description, 'MPL') ~ 'MAPLE',
                                str_detect(product_description, 'CORN') ~ 'CORNSYRUP',
                                str_detect(product_description, 'MOLAS') ~ 'MOLASSES',
                                str_detect(product_description, 'BLACKSTRA') ~ 'MOLASSES',
                                str_detect(product_description, 'SORGHUM') ~ 'SORGHUM',
                                str_detect(product_description, 'SUGAR') ~ 'SUGARFREE',
                                str_detect(product_description, 'SUGR') ~ 'SUGARFREE',
                                str_detect(product_description, 'CANE') ~ 'CANE', 
                                str_detect(product_description, 'MARIN') ~ 'MARINARA',
                                str_detect(product_description, 'VODKA ') ~ 'VODKA',
                                str_detect(product_description, 'MEATLESS') ~ 'MEATLESS',
                                str_detect(product_description, 'MEAT') ~ 'MEAT',
                                str_detect(product_description, 'SAUSAGE') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUS ') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUS/') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUSG') ~ 'SAUSAGE',
                                str_detect(product_description, 'ALFREDO') ~ 'ALFREDO',
                                str_detect(product_description, 'ALFEDO') ~ 'ALFREDO',
                                str_detect(product_description, 'ALFRDO') ~ 'ALFREDO',
                                str_detect(product_description, 'PUTTAN') ~ 'PUTTANESCA',
                                str_detect(product_description, 'PESTO') ~ 'PESTO',
                                str_detect(product_description, 'MSHRM') ~ 'MUSHROOM',
                                str_detect(product_description, 'MUSH') ~ 'MUSHROOM',
                                str_detect(product_description, 'MUSRM') ~ 'MUSHROOM',
                                str_detect(product_description, 'MSHR') ~ 'MUSHROOM',
                                str_detect(product_description, 'CLAM') ~ 'CLAM', 
                                str_detect(product_description, 'VEG') ~ 'VEG',
                                str_detect(product_description, 'GARDEN') ~ 'VEG',
                                str_detect(product_description, 'GRDN') ~ 'VEG',
                                str_detect(product_description, 'ONION') ~ 'VEG',
                                str_detect(product_description, 'ARTICHOKE') ~ 'VEG',
                                str_detect(product_description, 'EGGPLANT') ~ 'VEG',
                                str_detect(product_description, 'OLIVE') ~ 'VEG',
                                str_detect(product_description, 'TOM') ~ 'VEG',
                                str_detect(product_description, 'CHEESE') ~ 'CHEESE',
                                str_detect(product_description, 'PARM') ~ 'CHEESE',
                                str_detect(product_description, 'CHEDDAR') ~ 'CHEESE',
                                str_detect(product_description, 'CHDR') ~ 'CHEESE',
                                str_detect(product_description, 'FET') ~ 'FETTUCINI',
                                str_detect(product_description, 'EGG ') ~ 'EGG',
                                str_detect(product_description, 'ANGEL') ~ 'ANGEL',
                                str_detect(product_description, 'ROTI') ~ 'ROTINI',
                                str_detect(product_description, 'LASA') ~ 'LASAGNA',
                                str_detect(product_description, 'ELBO') ~ 'ELBOW',
                                str_detect(product_description, 'MACARON') ~ 'ELBOW',
                                str_detect(product_description, 'MANI') ~ 'MANICOTTI',
                                str_detect(product_description, 'ORZO') ~ 'ORZO',
                                str_detect(product_description, 'CAPE') ~ 'CAPELLINI',
                                str_detect(product_description, 'NACH') ~ 'SPINACH',
                                str_detect(product_description, 'BOW') ~ 'BOWTIE',
                                str_detect(product_description, 'VERMI') ~ 'VERMICELLI',
                                str_detect(product_description, 'GNOC') ~ 'GNOCCHI',
                                str_detect(product_description, 'ZIT') ~ 'ZITI',
                                str_detect(product_description, 'FAR') ~ 'FARFALLE',
                                str_detect(product_description, 'TORT') ~ 'TORTELLINI',           
                                str_detect(product_description, 'ROTE') ~ 'ROTELLE',
                                str_detect(product_description, 'MOSTA') ~ 'MOSTACCIOLI',
                                str_detect(product_description, 'ALPHA') ~ 'ALPHABET',
                                str_detect(product_description, 'RIGAT') ~ 'RIGATONI',
                                str_detect(product_description, 'SHELL') ~ 'SHELLS',
                                str_detect(product_description, 'PENNE') ~ 'PENNE',
                                str_detect(product_description, 'LIN')
                                   & commodity == 'PASTA' ~ 'LINGUINI',
                                str_detect(product_description, 'THIN') ~ 'THIN',
                                str_detect(product_description, 'SPAG') 
                                    & commodity=='PASTA'  ~ 'SPAGHETTI',
                                TRUE ~ 'OTHER' ))

product <- product %>%
                  select('product_description', 'product_attribute', 'product_size', everything())
```


For product_size, I removed spaces, recalculated all values to ounces for comparison purposes, cleaned up labels/values, and coerced the final results to numeric. Initially there were `r length(unique(product$product_size))` different values for product sizes.


```{r initial_product_size, message = FALSE, warning = FALSE}
# Initial product_size values
unique(sort(product$product_size))
```

```{r tidy_product_size, message = FALSE, warning = FALSE} 
# Tidying product$product_size
product <- mutate(product, 
           
           # remove spaces
           product_size = str_replace_all(product_size, ' ', ''),
           
           # Convert pounds to ounces           
           product_size = str_replace(product_size, '1LB', '16'),
           product_size = str_replace(product_size, '2LB', '36'),
           product_size = str_replace(product_size, '3LB', '48'),
           product_size = str_replace(product_size, '4LB', '64'),
           product_size = str_replace(product_size, '6LB11OZ', '26'), #Ragu Trad Spaghetti
           product_size = str_replace(product_size, 'GAL', '16'),
           product_size = str_replace(product_size, 'P 1 LB ', '16'),
           product_size = str_replace(product_size, 'N 1 LB', '16'),
           product_size = str_replace(product_size, '1.5LB', '24'),
           
           # Clean Up these sizes
           product_size = str_replace(product_size,'KH#13384', '16'),
           product_size = str_replace(product_size,'KH#18277', '8'),
           product_size = str_replace(product_size,'KH#18280', '8'),
           product_size = str_replace(product_size,'KH#18283', '8'),
           product_size = str_replace(product_size,'KH#20749', '16'),
           product_size = str_replace(product_size,'KH#2793', '12'),
           product_size = str_replace(product_size,'KH#39724', '8'),
           product_size = str_replace(product_size,'KH#58442', '24'),
           product_size = str_replace(product_size,'KH#61779', '24'),
           product_size = str_replace(product_size,'KH#61780', '24'),
           product_size = str_replace(product_size,'KH#68255', '3'),
           product_size = str_replace(product_size,'KH#6862', '16'),
           product_size = str_replace(product_size,'KH#69333', '12'),
           product_size = str_replace(product_size,'KH#71916', '26'),
           product_size = str_replace(product_size,'KH#8525', '16'),
           product_size = str_replace(product_size,'KH#8623', '16'),
           product_size = str_replace(product_size,'KH#8627', '16'),
           product_size = str_replace(product_size,'KH#8651', '16'),
           product_size = str_replace(product_size,'KH#8652', '16'),
           product_size = str_replace(product_size,'%KH#29483', '26'),
           product_size = str_replace(product_size,'%KH#9390', '7'),
           product_size = str_replace(product_size,'NOTAG', '16'),
           product_size = str_replace(product_size,'##########', ''),
           product_size = str_replace(product_size,'CUSTREQST', '12'),
           product_size = str_replace(product_size, '61/2OZ', '6.5'),
           product_size = str_replace(product_size, '311/2OZ', '31.5'),
           
           # Remove these units codes
           product_size = str_replace(product_size, 'OUNCE', ''),
           product_size = str_replace(product_size, 'OZ', ''),
           product_size = str_replace(product_size, '0Z', ''),
           product_size = str_replace(product_size, 'Z', ''),
           product_size = str_replace(product_size, 'FL', ''),
           product_size = str_replace(product_size, 'FMLY', ''),
           product_size = str_replace(product_size, 'PET', ''),
           product_size = str_replace(product_size, 'CR', ''),
           product_size = str_replace(product_size, 'NOTAG', ''),     
           product_size = str_replace(product_size, 'N', ''),
           product_size = str_replace(product_size, 'P', ''),
           product_size = str_replace(product_size, 'SO', ''))

# SAS encodes nulls as blanks, fill in blanks with 16 (most common)
product$product_size[product$product_size == ''] <- '16'

# Coerce to numeric
product <- mutate(product, 
       product_size = as.numeric(product$product_size))
``` 

I then created a new variable called 'product_size_factor' to represent the relative size of products in each commodity. Because the product sizes are now numeric, I was can calculate the median and IQR for each commodity.  


```{r determine_size_splits, message = FALSE, warning = FALSE}

# Determine appropriate splits for product_size groups
commodity_split <- split(product, f = product$commodity)
pasta_df <- data.frame(commodity_split[1])
sauce_df <- data.frame(commodity_split[2])
pancake_df <- data.frame(commodity_split[3])
syrup_df <- data.frame(commodity_split[4])

# Summary statistics for product sizes
size_stats <- map(c(pasta_df, sauce_df, pancake_df, syrup_df), summary, na.rm=TRUE)
size_stats[c(3, 9, 15, 21)] #print only product_size stats

#clean environment
remove_objects <- c('commodity_split', 'pasta_df', 'sauce_df', 'pancake_df', 'syrup_df', 'size_stats')
rm(list = ls()[ls() %in% remove_objects])
```



```{r code_checking_commodity_sizes, include = FALSE, message = FALSE, warning = FALSE}
# Count instance of each product size by commodity
product %>%
  group_by(commodity, product_size) %>%
  tally()
```


```{r assign_size_label, message = FALSE, warning = FALSE}
# Create variable 'product_size_factor'
PASTA_SMALL <-9
PASTA_REGULAR <-17
SAUCE_SMALL <- 12
SAUCE_REGULAR <-26
PANCAKE_SMALL <-10
PANCAKE_REGULAR <-20
SYRUP_SMALL <- 16
SYRUP_REGULAR <-24

product <- product %>%
  mutate(product_size_factor = case_when(
        commodity == 'PASTA' & product_size <= PASTA_SMALL ~ 'SMALL',
        commodity == 'PASTA' & product_size <= PASTA_REGULAR~ 'REGULAR',
        commodity == 'PASTA' ~ 'JUMBO',
        commodity == 'SAUCE' & product_size <= SAUCE_SMALL ~ 'SMALL',
        commodity == 'SAUCE' & product_size <= SAUCE_REGULAR ~ 'REGULAR',
        commodity == 'SAUCE' ~ 'JUMBO',
        commodity == 'PANCAKE' & product_size <= PANCAKE_SMALL ~ 'SMALL',
        commodity == 'PANCAKE' & product_size <= PANCAKE_REGULAR ~ 'REGULAR',
        commodity == 'PANCAKE' ~ 'JUMBO',
        commodity == 'SYRUP' & product_size <= SYRUP_SMALL ~ 'SMALL',
        commodity == 'SYRUP' & product_size <= SYRUP_REGULAR ~ 'REGULAR',
        commodity == 'SYRUP' ~ 'JUMBO',
        TRUE ~ 'OTHER' ))

size_level = c('SMALL', 
               'REGULAR', 
               'JUMBO')
product <- mutate(product, 
                  product_size_factor = factor(product_size_factor, levels = size_level))

```
I used these statistics, counts of product size, and my domain knowledge to determine appropriate cut off values for 'small', 'regular', and 'jumbo' in each commodity type. The cutoffs I assigned are:

* For Pasta:  `r PASTA_SMALL ` and `r PASTA_REGULAR`

* for Sauce:  `r SAUCE_SMALL ` and `r SAUCE_REGULAR`

* for Pancake:  `r PANCAKE_SMALL ` and `r PANCAKE_REGULAR`

* for Syrup:  `r SYRUP_SMALL ` and `r SYRUP_REGULAR`


```{r product_size_contingency_table, message = FALSE, warning = FALSE}
# Product_size_factor counts
table(product$product_size_factor, product$commodity) %>%
  addmargins()

# Product_size_factor proportions
round(table(product$product_size_factor, product$commodity) %>%
  prop.table(margin = 2), 2)
```


Final cleanup of product file to prepare for modeling.

```{r}
#convert brand to uppercase to be consistend and factor so it can be used in models
product$brand = toupper(factor(product$brand))
```



The cleaned product table (included because there were a lot of changes to product data): 

```{r cleaned_prod_table, echo = FALSE, message = FALSE, warning = FALSE}
# Cleaned product data
datatable(product %>% 
  select(upc, commodity, product_attribute, brand, product_description, product_size, product_size_factor, everything()) %>%
  arrange(desc(commodity), upc))
```



### 3.4 Join Tables

**The final prep step is to join the data.** 

I evaluated possible issues, then joined the dataframes, then cleaned up the joined data by renaming, filling in nulls, and reordering the columns

**Determine possible issues with tables before joining**

Transactions and Products are joined using 'upc'

* There are `r transactions %>% anti_join(product) %>% tally()` transactions which are not associated with a product and will result in NA in the product information when they are joined. I will not keep these unmatched transactions as they do not add value to my analysis on products/promotion links.

* There are `r product %>% anti_join(transactions) %>% tally()` products which are not associated with a transaction. I will ignore these products (this will happen automatically by the join used). 


Transactions and Weekly are joined using 'upc', 'week', 'store', and 'geography'

* There are `r transactions %>% anti_join(weekly) %>% tally()` transactions which are not associated with a weekly observation. Many of these are because promotion information was not recorded before week 43 which is 40% of the time period. After the join, there will be missing values for feature_desc and display_desc in these rows. Because Weekly promotions were not collected before week 43, I will assign 'Pre_Collection' to those missing values. But for weeks after that I will assign 'Unknown' to missing values. While these missing values after week 42 may reflect there were not any promotions for that UPC, I did not make that assumption because there is already a category called 'Not On Feature'. 

* There are `r weekly %>% anti_join(transactions) %>% tally()` weekly observations which are not associated with a transaction. I will ignore these weeks (automatically by the format of the join).


Transactions and Store are joined using 'store'

* There are `r transactions %>% anti_join(store) %>% tally()` transactions which are not associated with a store. 

* There are `r store %>% anti_join(transactions) %>% tally()` stores which are not associated with a transaction. 


**Join**

```{r join_tables, message = FALSE, warning = FALSE}
# Join Tables
carbo <- transactions %>%
          inner_join(product, by = 'upc')  %>%
          left_join(weekly, by = c('upc', 'week', 'store', 'geography')) %>%
          left_join(store, by = 'store') %>%
          arrange(household, day, upc)
```

**Clean after joining**

I renamed columns to be clear, shorter, and consistent. 

```{r rename_clean, message = FALSE, warning = FALSE}
# Rename columns to be clear and consistent
setnames(carbo, 
         old = c('units', 'time_of_transaction', 'product_description',
                 'product_size', 'store_zip_code'), 
         new = c('quantity', 'time', 'product_desc', 'ounces', 'store_zip'))
```


After the join, there are missing values for feature_desc and display_desc which I will fill in with 'Pre_Collection' and 'Unknown'. Implementing this involves saving the current factor level, converting the column to character, assigning the new values where there were missing values, then refactoring the column.


```{r adjust_feature_display_factors, message = FALSE, warning = FALSE }
# Missing values as a result of transaction and weekly join

# Preserve feature levels
feature_levels <- levels(carbo$feature_desc)
display_levels <- levels(carbo$display_desc)

# Convert to character
carbo$feature_desc <- as.character(carbo$feature_desc)
carbo$display_desc <- as.character(carbo$display_desc)

# Code values before and after week 43
carbo$feature_desc <- ifelse(carbo$week < 43, 
                                   'Pre_Collection',
                                   carbo$feature_desc)
carbo$feature_desc <- ifelse(carbo$week >= 43 & is.na(carbo$feature_desc), 
                                   'Unknown',
                                   carbo$feature_desc)
carbo$display_desc <- ifelse(carbo$week < 43, 
                                   'Pre_Collection',
                                   carbo$display_desc)
carbo$display_desc <- ifelse(carbo$week >= 43 & is.na(carbo$display_desc), 
                                   'Unknown',
                                   carbo$display_desc)

# Refactor with new levels
carbo <- mutate(carbo, 
                  feature_desc = factor(feature_desc, levels = feature_levels),
                  display_desc = factor(display_desc, levels = display_levels))
```


```{r missing_val_check, include = FALSE, message = FALSE, warning = FALSE}
# Verify there are no more missing values
colSums(is.na(carbo))
```


I reordered the columns. 
 
```{r reorder_columns_clean, message = FALSE, warning = FALSE}
# Reorder columns
carbo <- carbo %>%
  select(household, week, feature_desc, display_desc, commodity, brand, product_attribute, 
         ounces, product_size_factor, upc, product_desc, coupon, quantity, dollar_sales, 
         day, time, basket, everything())
```


### 3.5 Summary of Cleaned Data

In the cleaned data set, each observation is a household's UPC from a specific day. This cleaned data set contains `r dim(carbo)[1]` observations and `r dim(carbo)[2]` variables.

**The key variables I will be looking at are:**

* **commodity** - which has 4 values: `r unique(sort(carbo$commodity))`

* **feature_desc** - which is a factor with 8 ordered levels: `r levels(carbo$feature_desc)`

* **product_desc** - which is the name of the products. There are `r length(unique(carbo$product_desc))` different products.

* **household** - there are `r length(unique(carbo$household))` unique households. 

```{r glimpse_clean, echo=FALSE, message = FALSE, warning = FALSE}
glimpse(carbo)
```

**The first 500 observations**

```{r, datatable_clean, echo=FALSE, message = FALSE, warning = FALSE, errors = FALSE}
# First 500 rows of the cleaned data
datatable(head(carbo, 500))
```


## 4 Exploratory Data Analysis


```{r, include = FALSE}
# Used in in development so I don't have to keep recreating the dataset from the start
carbo_backup <- carbo
```

```{r, include = FALSE}
# Used in in development so I don't have to keep recreating the dataset from the start
carbo <- carbo_backup
```



### Exploratory Data Analysis

This step combines visualization with transformation to ask and answer interesting questions about the data. We calculate statistics and make visuals to find trends, anomalies, patterns, or relationships among the data.



#### Frequency
Up to this point, I have included all observations, regardless of whether a weekly marketing campaign was recorded. At this point, I filter out the observations where weekly marketing campaign is unknown, so that I can zoom in on the known campaigns. 


```{r}
#Remove observations where weekly marketing unknown
carbo <- carbo %>%
  filter (!display_desc %in% c("Pre_Collection", "Unknown"))
```

I looked at the frequency of product attributes as well as location of products on weekly mailer and store displays. 


```{r}
carbo %>%
  ggplot(aes(fct_rev(fct_infreq(product_attribute)))) +
  geom_bar()+
  scale_x_discrete(name='Product Attribute') +
  scale_y_continuous(labels = comma) +
  ggtitle('Product Attribute Frequency') +
  coord_flip()
  
```

```{r}
carbo %>%
  ggplot(aes(fct_rev(fct_infreq(feature_desc)))) +
  geom_bar()+
  scale_x_discrete(name='Mailer Location') +
  scale_y_continuous(labels = comma) +
  ggtitle('Product Location In Weekly Mailer') +
  coord_flip()
```



```{r}
carbo %>%
  ggplot(aes(fct_rev(fct_infreq(feature_desc)), fill = commodity)) +
  geom_bar()+
  scale_x_discrete(name='Mailer Location') +
  scale_y_continuous(labels = comma) +
  ggtitle('Product Location In Weekly Mailer (grouped by Commodity)') +
  facet_wrap(~ commodity)+
  coord_flip()
```


```{r}
carbo %>%
  ggplot(aes(fct_rev(fct_infreq(display_desc)))) +
  geom_bar()+
  scale_x_discrete(name='Display Location') +
  scale_y_continuous(labels = comma) +
  ggtitle('Product Location In-Store Display') +
  coord_flip()
```



```{r}
carbo %>%
  ggplot(aes(fct_rev(fct_infreq(display_desc)), fill=commodity)) +
  geom_bar()+
  scale_x_discrete(name='Display Location') +
  scale_y_continuous(labels = comma) +
  ggtitle('Product Location In-Store Display (grouped by Commodity)') +
  facet_wrap(~ commodity)+
  coord_flip()
```


#### Aggregate to the Product Grain

Next I looked at the data through the lens of product. I arranged the data so that one product is a single row in the dataframe. I summarized the quantity of sales, number baskets, number weeks, number stores, etc.

```{r message = FALSE, warning = FALSE, errors = FALSE}
#creating new dataframe at product grain: each observation (row) is a different upc
carbo_product = carbo %>%
                group_by(upc, commodity, brand, product_attribute, product_size_factor, product_desc) %>%
                summarize(num_households = n_distinct(household), 
                  percent_coupons = mean(coupon), 
                  total_quantity = sum(quantity), 
                  avg_quantity = mean(quantity),
                  max_quantity = max(quantity),
                  num_baskets = n_distinct(basket),
                  num_weeks = n_distinct(week),
                  first_week = min(week),
                  last_week = max(week),
                  avg_dollars = round(mean(dollar_sales),2),
                  total_dollars = round(sum(dollar_sales),2),
                  num_stores = n_distinct(store),
                  num_regions = n_distinct(geography)
                  )
```

Then I counted the number of times the product(upc) was part of specific locations in the weekly mailer and in-store displays. 

```{r}
#original of adding counts feature levels
level_names <- carbo %>%
    pull(feature_desc) %>%
    levels() 

level_names_underscore <- str_replace_all(level_names," ","_") #replace spaces

for (i in seq_along(level_names_underscore)) {
  variable_name_new <- paste0("Num_Feature_", level_names_underscore[i])
  carbo_product1 <- carbo %>% group_by(upc) %>%
                summarize(!!variable_name_new := sum(feature_desc == level_names[i]))
  
  carbo_product <- carbo_product%>%
          left_join(carbo_product1, by = 'upc')
}

rm(carbo_product1)
```


```{r}
#original of adding counts display levels
level_names <- carbo %>%
    pull(display_desc) %>%
    levels() 

level_names_underscore <- str_replace_all(level_names," ","_") #replace spaces

for (i in seq_along(level_names_underscore)) {
  variable_name_new <- paste0("Num_Display_", level_names_underscore[i])
  carbo_product1 <- carbo %>% group_by(upc) %>%
                summarize(!!variable_name_new := sum(display_desc == level_names[i]))
  
  carbo_product <- carbo_product%>%
          left_join(carbo_product1, by = 'upc')
}

rm(carbo_product1)
```


making into function of adding counts display and feature levels
```{r }
# attempt at making function to count feature and display levels 
# I can't figure out how to pass the column into the function. ################
# To pass it for level names, I could get it to work if I passed df$col
# But later in the summarize function it doesn't work.
# Is there a way to rewrite this to group and summarize with a column I have passed into function?

factor_levels <- function(column, prefix="Num_") {
  #replaces spaces from factor levels
  level_names <- levels(column) #this column works
  level_names_underscore <- str_replace_all(level_names," ","_") 

  #iterates through factor levels, creating a field named after campaign and counting per upd
  for (i in seq_along(level_names_underscore)) {
    variable_name_new <- paste0(prefix, level_names_underscore[i])
    carbo_product1 <- carbo %>% group_by(upc) %>%
                summarize(!!variable_name_new := sum(column == level_names[i])) #this column doesn't
    carbo_product <- carbo_product%>%
          left_join(carbo_product1, by = 'upc')
  }
}

factor_levels(column = carbo$feature_desc, prefix = "Num_Feature1_")
factor_levels(column = carbo$display_desc, prefix = "Num_Display1_")
```

Top three products by quantity sold.

```{r}
carbo %>% top_n(3, quantity) 
```


#### Clustering Products

Clustering is a form of EDA where observations are divided into meaningful groups that share common characteristics (features). Cluster members are similar to one another, and distinctly different from others outside the cluster. I clusterd the product dataframe so that I could compare groups.

```{r message = FALSE, warning = FALSE, errors = FALSE}
#fitting the model
dist_carbo <- dist(carbo_product, method='binary') #jiccard distance used for categorical
hc_carbo <- hclust(dist_carbo, method='complete') #linkage
cluster_assignment <- cutree(hc_carbo, k = 3) #extract cluster assignment
carbo_product$cluster = cluster_assignment #append assignment to original dataframe
```

Product Grain Dataframe Showing Cluster Assignments

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Product Grain dataframe with cluster assignment
datatable(carbo_product %>%
  select (cluster, everything()) %>%
  arrange(cluster) )
```


It's hard to visualize clusters with more than two variables. But we can use summary statistics of features in order to build a narrative of what makes the observations similar to each other within the cluster while different from observations in other clusters. Each row is a different cluster and the columns are summary statistics for that cluster. 

```{r message = FALSE, warning = FALSE, errors = FALSE}
#statistics on features clustered
clustered_stats = carbo_product %>% 
  group_by(cluster) %>% 
  summarise_all(funs(round(mean(.),2)))

datatable(clustered_stats)
```

We can also visualze with a dendrogram, where the height of each branch is determined by linkage and distance decisions we made during clustering. The x-axis plots observations, so we see that using 3 clusters, one is very small only including a few observations (because it is narrow). The members appear to be tightly grouped, but different from one another. The labels are hard to read as there are 900 observations.

```{r message = FALSE, warning = FALSE, errors = FALSE}
#colored dendrogram to show cluster height
dend_clustered <- as.dendrogram(hc_carbo) #converts hclust to dendrogram
dend_colored <- color_branches(dend_clustered, k=3)
plot(dend_colored)
```


```{r}
# Plot the relationship between two selected variable with color showing assigned cluster
ggplot(carbo_product, aes( x = product_attribute, y = avg_quantity, color = factor(cluster))) + 
    geom_line(aes(group = product_attribute))
```


## 5 Summary


### Business Question Addressed

Summarize problem statement address. How addressed (methodology) 

### Interesting Insights


### Implications


### Limitations

An extension of this analysis could include the impact of additional ways of engaging with customers, such as through other Kroger marketing channels and digital ads across the web. Another extension could look at the specifics of coupons redeemed as to whether they were a result of customer specific mailers, general vendor coupons (such as from the Sunday paper), or digital coupons from the Kroger app. 
