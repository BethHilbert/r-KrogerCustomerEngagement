---
title: "Promotions and Sales of Carbohydrate Products"
author: "Beth Hilbert"
date: "`r Sys.Date()`"
output:
  html_document
---

#{.tabset .tabset-fade}

## 1 Introduction

### Business Question
My goal is to analyze how weekly mailers and in-store displays correlate with sales 
for carbohydrate products. I will measure the success of a promotion by the number 
of shopping baskets that include the product. 

### Plan
I will use the Carbo_Loading [dataset](http://8451.com/area51) provided by 84.51. 

The important fields I will use are feature description (describing weekly mailers), 
display description (describing in-store promo displays), attribute (a field I 
created for grouping products such as maple, macaroni, alfredo), and commodity 
(pasta, sauce, pancake, syrup). 


I will look at the data two ways. First I will look at promotion types and resulting 
sales by commodity and attribute. Then I will aggregate the product information, 
segment these products into groups with a hierarchical clustering model, and 
analyze what differentiates the resulting clusters. I will look at how these 
clusters differ in their response to promotions and brands purchased.

The result will be some specific recommendations of promotion/commodity 
combinations to continue in order to keep market share and some new 
promotion/commodity combinations to drive new sales.

### Benefit
The benefit of the analysis will be to see whether certain combinations of products 
and promotions are more effective in producing sales. By understanding the types 
of promotions in which products are effective and the clusters which are most 
affected by promotions, we can improve the customer experience and drive business 
results.

## 2 Packages

### Packages

The following packages are required to reproduce these results: 

```{r load_packages, message = FALSE, warning = FALSE, error = FALSE}
# Load Required Packages
library(DT) ## create functional tables in HTML
library(gridExtra) ## plot more than one graph
library(haven) ## read statistical software data (original data is stored in SAS files)
library(cluster) ## plot to decide best number of clusters
library(dendextend) ## for use plotting dendrogram with clustering models
library(scales) ## format labels with dollars and commas
library(tidyverse) ## collection of R data science packages 
```



Loading Tidyverse loads a collection of data science packages. From Tidyverse I am using: 

* dplyr to manipulate data and join tables
* ggplot2 to visualize 
* purrr for functional programming 
* stringr to work with strings
* tidyr for tidying data. 

## 3 Data Preperation {.tabset .tabset-fade}

### 3.1 Load Data

**The first prep step is to understand the source of the data.**  I will use the 
Carbo_Loading dataset provided by 84.51. The 
[data](http://8451.com/area51) 
is a subset of four potentially related product commodities (pasta/sauce, 
pancake mix/syrup), capturing their sales and weekly promotions. Promotions are 
identified by location in weekly mailer, location of in-store display, and 
coupons redeemed. There are 927 products with their associated promotion 
campaigns over a 2 year period. The data includes 5,197,681 sale transactions 
with a grain of one row per household per UPC per day. This data set was prepared 
for classroom projects and case studies to allow students to interact with 
real-world data. 


Because the data is stored in a SAS file format (.sas7bdat), there are a few 
unique things to consider:

* The data is loaded using the read_sas command from the Haven package. 

* Some variables included attributes which allow them to later be read back into 
SAS. However these attributes were only included on some of the key fields, which 
caused errors when joining the tables together. For example the 'upc' variable 
had attributes in the transactions and weekly tables, but not in product table. 
I removed the attributes with zap_formats(). 

* SAS encodes nulls as blanks, so nulls are not identifiable with the typical 
is.na() function and do not show on summary() as missing values. The only nulls 
in the original datasets (encoded as blank strings) were in the product_size 
variable. However more nulls were introducted during the join process.


```{r load_data, message = FALSE, warning = FALSE, error = FALSE}
# Load data
files <- c('transactions', 'product_lookup', 'causal_lookup', 'store_lookup')
df_names <- c('transactions', 'product', 'weekly', 'store')

for (i in seq_along(files)) {
  # Create file path
  full_path <- paste0('data/',files[i], '.sas7bdat')

  # Import data
  if (file.exists(full_path)) {
   df <- read_sas(full_path)
   df <- zap_formats(df) 
   assign(df_names[i],df)
   rm(df)
  } else {
    response <- paste('There is no file with the name', full_path)
    print(response)
  }
}
```


### 3.2 Initial Data Structure

**The second prep step is to understand the initial structure of the data.**  
The primary table is transactions and there are 3 lookup tables for products, 
stores, and weekly promotions.

* **Transaction Table** contains a sample of 2 years of carbo related transactions 
at the household level. The grain of the table is one row per household per UPC 
per day. There are `r comma(dim(transactions)[1])` transaction rows and 
`r dim(transactions)[2]` variables. The variables are named: `r names(transactions)`.

* **Product Table** provides detailed product information for each UPC. There 
are `r dim(product)[1]` product rows and `r dim(product)[2]` variables. The 
variables are named: `r names(product)`. 

* **Weekly Table** provides weekly promotion activity for each UPC. In the weekly 
table, there are `r comma(dim(weekly)[1])` UPC weekly promotion rows 
and `r dim(weekly)[2]` variables. The variables are named: `r names(weekly)`. 

* **Store Table** records each store's zip code. There are `r dim(store)[1]` 
store rows and `r dim(store)[2]` variables. The variables are named: `r names(store)`. 


**Comments on initial data structure:**

* Households are uniquely identified.

* Time is recorded by 'time_of_transaction'' expressed as 24 hour clock, 'week' 
expressed chronologically from 1 to 104, and 'day' expressed from 1 to 728. There 
is not an actual calendar date, only a relative chronological order of transactions.

* Geography is divided into two large regions encompassing portions of several 
states.

* Coupon indicates whether a coupon was used with the UPC on that day for that 
household.  

* Commodity refers to the 4 categories (Pasta, Pasta Sauce, Pancake Mix, Syrup). 

* Feature_desc describes the location of product on a weekly mailer.

* Display_desc describes location of in-store display containing the product.


```{r glimpse_initial_data, message = FALSE, warning = FALSE, error = FALSE}
# Glimpse tables
for (i in seq_along(df_names)) {
  print(df_names[i])
  glimpse(get(df_names[i]))
}
```


### 3.3 Tidy the Data

**The third prep step is to look at the data.** 

Decisions based on data are dependent on data quality. A large part of time is 
spent preparing the data for analysis. We have to decide which data to gather and 
what features of the data are important. This step also considers what data is 
missing and how to create new variables and summaries to reflect these gaps. 

To begin with, I verified the data was stored in a 'tidy' format in which 
observations are recorded in rows (measure same unit across all attributes with 
one measurement per row), variables are columns (measure same attribute across 
observations), and there is one type of observational unit per table. 

I then looked at the values and distributions of each variable using summary(df) 
and unique(sort(df$var)). I was looking for anything unusual such as variations 
on spelling, capitalization, missing, or unexpected values. As a result, I made 
a few changes to the transactions, products and weekly promotion tables. No 
changes were made to the store table.


**Tidying transaction data**

I looked at the general distribution of the transaction data. There are 
`r comma(n_distinct(transactions$household)) ` unique households. There is an 
average of `r round((n_distinct(transactions$basket) / n_distinct(transactions$household)),2) ` 
baskets per household over the 2 year period. This seems like a very small number 
of grocery trips. This is indicative to me that when the data was sampled, the 
focus was not on preserving transactions for an entire household. Because of this, 
I determined the data would not support analyzing a specific customer's response 
to a campaign. So I changed the grain of my review from looking at transactions 
by individual households to looking at products.  

Coupons were used on products in `r round(mean(transactions$coupon)*100,2) ` % 
of the transactions. I do not know the industry standard, but it seems reasonable.

I then evaluated whether the dollar sales were reasonable. The sales ranged 
from $`r range(transactions$dollar_sales)[1] ` to $`r range(transactions$dollar_sales)[2] ` 
with a median sale of $`r median(transactions$dollar_sales)`.


```{r trans_dollars, results= FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Look closer at transaction$dollar_sales
negative_dollars <- transactions[transactions$dollar_sales < 0,]
count_negative_dollars <- count(negative_dollars)
percent_negative_dollars <- round((count(negative_dollars) / count(transactions))*100, 4)

zero_dollars <- transactions[transactions$dollar_sales == 0,]
count_zero_dollars <- count(zero_dollars)
percent_zero_dollars <- round((count(zero_dollars) / count(transactions))*100, 4)

# Evaluate how prevalent negative sales are
negative_dollars %>%
  group_by(upc) %>%
  tally() %>%
  arrange(desc(n))
```

* There were `r count_negative_dollars` sales that were negative which represents
`r percent_negative_dollars`% of total transactions. I assumed negatives were 
appropriate as there could be returns. I initially considered removing these 
rows with negative sales from the data since returning an item would be unaffected 
by advertising or product placement in the store. But I decided to keep this data 
to reflect net sales in products which were frequently returned.

* There were `r count_zero_dollars` sales that were $0 which represents 
`r percent_zero_dollars`% of total transactions. I kept these rows since sales 
of $0 could be the result of a promotion.

* There were some dollar values as high as $`r max(transactions$dollar_sales) ` 
and units as high as `r max(transactions$units) ` within a single basket. I used 
a boxplot to compare dollar sales to units and determined they were most likely 
within an expected range (more units purchased resulted in more dollars spent).


```{r boxplot_unusual_transactions, message = FALSE, warning = FALSE, error = FALSE}
# Plot potentially unusual transactions
boxplot(transactions$dollar_sales ~ transactions$units, 
        main = 'Potentially Unusual Transactions', 
        xlab = 'Quantity of UPC Per Basket', 
        ylab = 'Dollars of UPC Per Basket')

```

I made a few changes to the transaction data:

* time_of_transactions - I coerced to numeric so it could be easily compared. I 
did not use a standard date format because other time related information is 
recorded chronologically, ignoring calendar date.

* week is in both transaction table and weekly table. The transaction table weeks 
range from 1 to 104, but the weekly table starts at week 43. In my initial 
analysis, I kept all 104 weeks. But I later decided to only keep the transactions 
for which there is weekly promotion information. 


```{r tidy_transactions, message = FALSE, warning = FALSE, error = FALSE}
# Coerce time to numeric
transactions$time_of_transaction <- as.numeric(transactions$time_of_transaction)
```


```{r clean_trans_environment, message = FALSE, warning = FALSE, error = FALSE}
# Clean environment
remove_objects <- c('negative_dollars', 'count_negative_dollars', 
                    'percent_negative_dollars', 'zero_dollars', 
                    'count_zero_dollars', 'percent_zero_dollars' )
rm(list = ls()[ls() %in% remove_objects])
```


**Tidying weekly promotion data**

In the weekly promotion data there are `r length(unique(weekly$feature_desc))` 
descriptions of weekly mailer and `r length(unique(weekly$display_desc))` 
descriptions of in-store display. I coerced both of these to factors. I also 
replaced hyphens and backslashes with underscores. 

* feature_desc represents the weekly mailer promotions. I created factor levels 
with the assumption that items on the advertising wrap are more important than 
covers, which are more important than not featured items. 

* display_desc represents the in-store promotion displays. I created factor 
levels with the assumption that end caps are better than aisles, and front of 
the store is better than rear. I will later use these levels as variable names, 
so I changed hyphens and slashes to underscores

```{r tidy_weekly, message = FALSE, warning = FALSE, error = FALSE} 
# Factor features and display descriptions
# Change hyphen and slashes to underscore 
weekly$display_desc <- str_replace_all(weekly$display_desc,'-','_')
weekly$display_desc <- str_replace_all(weekly$display_desc,
                              'Promo/Seasonal Aisle','Promo_Seasonal Aisle')

# Factor feature and display descriptions
feature_levels = c('Wrap Front Feature', 
                  'Wrap Back Feature', 
                  'Wrap Interior Feature',
                  'Front Page Feature', 
                  'Back Page Feature',
                  'Interior Page Feature', 
                  'Interior Page Line Item',
                  'Not on Feature')

display_levels = c('Store Front',
                  'Front End Cap',
                  'Side_Aisle End Cap',
                  'Promo_Seasonal Aisle',
                  'Secondary Location Display',
                  'Rear End Cap',
                  'Store Rear',
                  'Mid_Aisle End Cap',
                  'In_Aisle',
                  'In_Shelf',
                  'Not on Display')

weekly <- mutate(weekly, 
    feature_desc = factor(feature_desc, ordered = TRUE, levels = feature_levels),
    display_desc = factor(display_desc, ordered = TRUE, levels = display_levels))
``` 


**Tidying product data**

I made a lot of changes to the product data.

I renamed and factored the commodities.

```{r tidy_commodity, message = FALSE, warning = FALSE, error = FALSE}
# Rename and factor commodities
product <- product %>%
  mutate(commodity = case_when(
                                str_detect(commodity, 'pasta sauce') ~ 'SAUCE',
                                str_detect(commodity, 'pasta') ~ 'PASTA',
                                str_detect(commodity, 'pancake mixes') ~ 'PANCAKE',
                                str_detect(commodity, 'syrups') ~ 'SYRUP',
                                TRUE ~ NA_character_))

commodity_level = c('PASTA', 
                  'SAUCE', 
                  'PANCAKE',
                  'SYRUP')

product <- mutate(product, 
                  commodity = factor(commodity, levels = commodity_level))
```

I created a variable named 'product_attribute' which identifies a characteristic 
in the product description to group by (such as buttermilk, maple, fettucini). 
This cleaned inconsistent product names so that similar products can be compared. 

```{r create_product_attribute, message = FALSE, warning = FALSE, error = FALSE}
# Create variable 'product_attribute' for grouping and renaming descriptions consistently
product <- product %>%
  mutate(product_attribute = case_when(
                                str_detect(product_description, 'BUTTERM') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTMK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTRMK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'BTRMLK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'MILK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'MLK') ~ 'BUTTERMILK',
                                str_detect(product_description, 'LITE') ~ 'LITE',
                                str_detect(product_description, 'BELG') ~ 'BELGIAN',
                                str_detect(product_description, 'BUCKWHEAT') ~ 'BUCKWHEAT',
                                str_detect(product_description, 'BLUEB') ~ 'BERRY',
                                str_detect(product_description, 'BERRY') ~ 'BERRY',
                                str_detect(product_description, 'BLUBRY') ~ 'BERRY',
                                str_detect(product_description, 'MAPLE') ~ 'MAPLE',
                                str_detect(product_description, 'MPL') ~ 'MAPLE',
                                str_detect(product_description, 'CORN') ~ 'CORNSYRUP',
                                str_detect(product_description, 'MOLAS') ~ 'MOLASSES',
                                str_detect(product_description, 'BLACKSTRA') ~ 'MOLASSES',
                                str_detect(product_description, 'SORGHUM') ~ 'SORGHUM',
                                str_detect(product_description, 'SUGAR') ~ 'SUGARFREE',
                                str_detect(product_description, 'SUGR') ~ 'SUGARFREE',
                                str_detect(product_description, 'CANE') ~ 'CANE', 
                                str_detect(product_description, 'MARIN') ~ 'MARINARA',
                                str_detect(product_description, 'VODKA ') ~ 'VODKA',
                                str_detect(product_description, 'MEATLESS') ~ 'MEATLESS',
                                str_detect(product_description, 'MEAT') ~ 'MEAT',
                                str_detect(product_description, 'SAUSAGE') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUS ') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUS/') ~ 'SAUSAGE',
                                str_detect(product_description, 'SAUSG') ~ 'SAUSAGE',
                                str_detect(product_description, 'ALFREDO') ~ 'ALFREDO',
                                str_detect(product_description, 'ALFEDO') ~ 'ALFREDO',
                                str_detect(product_description, 'ALFRDO') ~ 'ALFREDO',
                                str_detect(product_description, 'PUTTAN') ~ 'PUTTANESCA',
                                str_detect(product_description, 'PESTO') ~ 'PESTO',
                                str_detect(product_description, 'MSHRM') ~ 'MUSHROOM',
                                str_detect(product_description, 'MUSH') ~ 'MUSHROOM',
                                str_detect(product_description, 'MUSRM') ~ 'MUSHROOM',
                                str_detect(product_description, 'MSHR') ~ 'MUSHROOM',
                                str_detect(product_description, 'CLAM') ~ 'CLAM', 
                                str_detect(product_description, 'VEG') ~ 'VEG',
                                str_detect(product_description, 'GARDEN') ~ 'VEG',
                                str_detect(product_description, 'GRDN') ~ 'VEG',
                                str_detect(product_description, 'ONION') ~ 'VEG',
                                str_detect(product_description, 'ARTICHOKE') ~ 'VEG',
                                str_detect(product_description, 'EGGPLANT') ~ 'VEG',
                                str_detect(product_description, 'OLIVE') ~ 'VEG',
                                str_detect(product_description, 'TOM') ~ 'VEG',
                                str_detect(product_description, 'CHEESE') ~ 'CHEESE',
                                str_detect(product_description, 'PARM') ~ 'CHEESE',
                                str_detect(product_description, 'CHEDDAR') ~ 'CHEESE',
                                str_detect(product_description, 'CHDR') ~ 'CHEESE',
                                str_detect(product_description, 'FET') ~ 'FETTUCINI',
                                str_detect(product_description, 'EGG ') ~ 'EGG',
                                str_detect(product_description, 'ANGEL') ~ 'ANGEL',
                                str_detect(product_description, 'ROTI') ~ 'ROTINI',
                                str_detect(product_description, 'LASA') ~ 'LASAGNA',
                                str_detect(product_description, 'ELBO') ~ 'ELBOW',
                                str_detect(product_description, 'MACARON') ~ 'ELBOW',
                                str_detect(product_description, 'MANI') ~ 'MANICOTTI',
                                str_detect(product_description, 'ORZO') ~ 'ORZO',
                                str_detect(product_description, 'CAPE') ~ 'CAPELLINI',
                                str_detect(product_description, 'NACH') ~ 'SPINACH',
                                str_detect(product_description, 'BOW') ~ 'BOWTIE',
                                str_detect(product_description, 'VERMI') ~ 'VERMICELLI',
                                str_detect(product_description, 'GNOC') ~ 'GNOCCHI',
                                str_detect(product_description, 'ZIT') ~ 'ZITI',
                                str_detect(product_description, 'FAR') ~ 'FARFALLE',
                                str_detect(product_description, 'TORT') ~ 'TORTELLINI',
                                str_detect(product_description, 'ROTE') ~ 'ROTELLE',
                                str_detect(product_description, 'MOSTA') ~ 'MOSTACCIOLI',
                                str_detect(product_description, 'ALPHA') ~ 'ALPHABET',
                                str_detect(product_description, 'RIGAT') ~ 'RIGATONI',
                                str_detect(product_description, 'SHELL') ~ 'SHELLS',
                                str_detect(product_description, 'PENNE') ~ 'PENNE',
                                str_detect(product_description, 'LIN')
                                   & commodity == 'PASTA' ~ 'LINGUINI',
                                str_detect(product_description, 'THIN') ~ 'THIN',
                                str_detect(product_description, 'SPAG') 
                                    & commodity=='PASTA'  ~ 'SPAGHETTI',
                                TRUE ~ 'OTHER' ))

product <- product %>%
              mutate(product_attribute = factor(product_attribute)) %>%
              select('product_description', 'product_attribute', 'product_size', everything())
```


For product size, I removed spaces, recalculated all values to ounces for 
comparison purposes, cleaned up labels/values, and coerced the final results to 
numeric. For unusual size coding (such as KH#13384) I looked at the product 
description and commodity to determine an appropriate size. Initially there were
`r length(unique(product$product_size))` different values for product sizes.


```{r initial_product_size, message = FALSE, warning = FALSE, error = FALSE}
# Initial product_size values
unique(sort(product$product_size))
```

```{r tidy_product_size, message = FALSE, warning = FALSE, error = FALSE} 
# Tidying product$product_size
product <- mutate(product, 
           
           # Remove spaces
           product_size = str_replace_all(product_size, ' ', ''),
           
           # Convert pounds to ounces           
           product_size = str_replace(product_size, '1LB', '16'),
           product_size = str_replace(product_size, '2LB', '36'),
           product_size = str_replace(product_size, '3LB', '48'),
           product_size = str_replace(product_size, '4LB', '64'),
           product_size = str_replace(product_size, '6LB11OZ', '26'), #Ragu Trad Spaghetti
           product_size = str_replace(product_size, 'GAL', '16'),
           product_size = str_replace(product_size, 'P 1 LB ', '16'),
           product_size = str_replace(product_size, 'N 1 LB', '16'),
           product_size = str_replace(product_size, '1.5LB', '24'),
           
           # Clean these sizes
           product_size = str_replace(product_size,'KH#13384', '16'),
           product_size = str_replace(product_size,'KH#18277', '8'),
           product_size = str_replace(product_size,'KH#18280', '8'),
           product_size = str_replace(product_size,'KH#18283', '8'),
           product_size = str_replace(product_size,'KH#20749', '16'),
           product_size = str_replace(product_size,'KH#2793', '12'),
           product_size = str_replace(product_size,'KH#39724', '8'),
           product_size = str_replace(product_size,'KH#58442', '24'),
           product_size = str_replace(product_size,'KH#61779', '24'),
           product_size = str_replace(product_size,'KH#61780', '24'),
           product_size = str_replace(product_size,'KH#68255', '3'),
           product_size = str_replace(product_size,'KH#6862', '16'),
           product_size = str_replace(product_size,'KH#69333', '12'),
           product_size = str_replace(product_size,'KH#71916', '26'),
           product_size = str_replace(product_size,'KH#8525', '16'),
           product_size = str_replace(product_size,'KH#8623', '16'),
           product_size = str_replace(product_size,'KH#8627', '16'),
           product_size = str_replace(product_size,'KH#8651', '16'),
           product_size = str_replace(product_size,'KH#8652', '16'),
           product_size = str_replace(product_size,'%KH#29483', '26'),
           product_size = str_replace(product_size,'%KH#9390', '7'),
           product_size = str_replace(product_size,'NOTAG', '16'),
           product_size = str_replace(product_size,'##########', ''),
           product_size = str_replace(product_size,'CUSTREQST', '12'),
           product_size = str_replace(product_size, '61/2OZ', '6.5'),
           product_size = str_replace(product_size, '311/2OZ', '31.5'),
           
           # Remove units codes
           product_size = str_replace(product_size, 'OUNCE', ''),
           product_size = str_replace(product_size, 'OZ', ''),
           product_size = str_replace(product_size, '0Z', ''),
           product_size = str_replace(product_size, 'Z', ''),
           product_size = str_replace(product_size, 'FL', ''),
           product_size = str_replace(product_size, 'FMLY', ''),
           product_size = str_replace(product_size, 'PET', ''),
           product_size = str_replace(product_size, 'CR', ''),
           product_size = str_replace(product_size, 'NOTAG', ''),     
           product_size = str_replace(product_size, 'N', ''),
           product_size = str_replace(product_size, 'P', ''),
           product_size = str_replace(product_size, 'SO', ''))

# SAS encodes nulls as blanks, fill in blanks with 16 (most common)
product$product_size[product$product_size == ''] <- '16'

# Coerce to numeric
product <- mutate(product, 
       product_size = as.numeric(product$product_size))
``` 

I then created a new variable called 'product_size_factor' to represent the 
relative size of products in each commodity. Because the product sizes are now 
numeric, I was able to calculate the median and IQR for each commodity.  


```{r determine_size_splits, message = FALSE, warning = FALSE, error = FALSE}
# Determine appropriate splits for product_size groups
commodity_split <- split(product, f = product$commodity)
pasta_df <- data.frame(commodity_split[1])
sauce_df <- data.frame(commodity_split[2])
pancake_df <- data.frame(commodity_split[3])
syrup_df <- data.frame(commodity_split[4])

# Summary statistics for product sizes by commodity
size_stats <- map(c(pasta_df, sauce_df, pancake_df, syrup_df), summary, na.rm = TRUE)
size_stats[c(3, 9, 15, 21)] #print only product_size stats
```


```{r check_commodity_size, include = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Count instance of each product size by commodity
product %>%
  group_by(commodity, product_size) %>%
  tally()
```


```{r assign_size_label, message = FALSE, warning = FALSE, error = FALSE}
# Create variable 'product_size_factor'
PASTA_SMALL <- 9
PASTA_REGULAR <- 17
SAUCE_SMALL <- 12
SAUCE_REGULAR <- 26
PANCAKE_SMALL <- 10
PANCAKE_REGULAR <- 20
SYRUP_SMALL <- 16
SYRUP_REGULAR <- 24

product <- product %>%
  mutate(product_size_factor = case_when(
        commodity == 'PASTA' & product_size <= PASTA_SMALL ~ 'SMALL',
        commodity == 'PASTA' & product_size <= PASTA_REGULAR~ 'REGULAR',
        commodity == 'PASTA' ~ 'JUMBO',
        commodity == 'SAUCE' & product_size <= SAUCE_SMALL ~ 'SMALL',
        commodity == 'SAUCE' & product_size <= SAUCE_REGULAR ~ 'REGULAR',
        commodity == 'SAUCE' ~ 'JUMBO',
        commodity == 'PANCAKE' & product_size <= PANCAKE_SMALL ~ 'SMALL',
        commodity == 'PANCAKE' & product_size <= PANCAKE_REGULAR ~ 'REGULAR',
        commodity == 'PANCAKE' ~ 'JUMBO',
        commodity == 'SYRUP' & product_size <= SYRUP_SMALL ~ 'SMALL',
        commodity == 'SYRUP' & product_size <= SYRUP_REGULAR ~ 'REGULAR',
        commodity == 'SYRUP' ~ 'JUMBO',
        TRUE ~ 'OTHER' ))

size_level = c('SMALL', 
               'REGULAR', 
               'JUMBO')
product <- mutate(product, 
           product_size_factor = factor(product_size_factor, levels = size_level))

```
I used statistics, counts, and my domain knowledge to determine appropriate cut 
off values for 'small', 'regular', and 'jumbo' in each commodity type. 
The cutoffs I assigned are:

* For Pasta:  `r PASTA_SMALL ` and `r PASTA_REGULAR`

* for Sauce:  `r SAUCE_SMALL ` and `r SAUCE_REGULAR`

* for Pancake:  `r PANCAKE_SMALL ` and `r PANCAKE_REGULAR`

* for Syrup:  `r SYRUP_SMALL ` and `r SYRUP_REGULAR`


```{r product_size_contingency_table, message = FALSE, warning = FALSE, error = FALSE}
# Product_size_factor counts
table(product$product_size_factor, product$commodity) %>%
  addmargins()

# Product_size_factor proportions
round(table(product$product_size_factor, product$commodity) %>%
  prop.table(margin = 2), 2)
```


The last thing I cleaned on the product data was to convert the brand to 
uppercase and then factor.

```{r brand_to_upper, message = FALSE, warning = FALSE, error = FALSE}
# Convert brand to uppercase and factor
product$brand = factor(toupper(product$brand))
```


Here's the cleaned product table (I include the product table because there 
were a lot of changes, other tables did not have as many modifications): 

```{r cleaned_prod_table, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Cleaned product data
datatable(product %>% 
  select(upc, commodity, product_attribute, brand, product_description, 
         product_size, product_size_factor, everything()) %>%
  arrange(desc(commodity), upc))
```

```{r clean_product_environ, message = FALSE, warning = FALSE, error = FALSE}
# Clean environment
remove_objects <- c('commodity_split', 'pasta_df', 'sauce_df', 'pancake_df', 
                    'syrup_df', 'size_stats', 'PASTA_SMALL', 'PASTA_REGULAR', 
                    'SAUCE_SMALL', 'SAUCE_REGULAR', 'PANCAKE_SMALL', 
                    'PANCAKE_REGULAR', 'SYRUP_SMALL', 'SYRUP_REGULAR')
rm(list = ls()[ls() %in% remove_objects])
```




### 3.4 Join Tables

**The final prep step is to join the data.** 

I evaluated possible issues, then joined the dataframes, then cleaned up the 
joined data by renaming and reordering the columns

**Determine possible issues with joining**

Transactions and Products are joined using 'upc'

* There are `r transactions %>% anti_join(product) %>% tally()` transactions 
which are not associated with a product. I will not keep these unmatched 
transactions as they do not add value to my analysis on products/promotion links.

* There are `r product %>% anti_join(transactions) %>% tally()` products which 
are not associated with a transaction. I will also ignore these products (this 
will happen automatically by the join used). 


Transactions and Weekly Promotions are joined using 'upc', 'week', 'store', and 'geography'

* There are `r transactions %>% anti_join(weekly) %>% tally()` transactions which 
are not associated with a weekly promotions. Many of these are because promotion 
information was not recorded before week 43. Because my focus is to evaluate the 
cases where I know the weekly promotion, I will remove all these unmatched 
transactions.  

* There are `r weekly %>% anti_join(transactions) %>% tally()` weekly promotions 
which are not associated with a transaction. I will ignore these weeks 
(automatically by the format of the join).


Transactions and Store are joined using 'store'

* There are `r transactions %>% anti_join(store) %>% tally()` transactions 
which are not associated with a store. 

* There are `r store %>% anti_join(transactions) %>% tally()` stores which are 
not associated with a transaction. 


**Join**

```{r join_tables, message = FALSE, warning = FALSE, error = FALSE}
# Join tables
carbo <- transactions %>%
          inner_join(product, by = 'upc')  %>%
          inner_join(weekly, by = c('upc', 'week', 'store', 'geography')) %>%
          left_join(store, by = 'store') %>%
          arrange(household, day, upc)
```

**Clean after joining**

I renamed columns to be clear, shorter, and consistent. 

```{r rename_clean, message = FALSE, warning = FALSE, error = FALSE}
# Rename columns to be clear and consistent
carbo <- rename(carbo, quantity = units, time = time_of_transaction, 
                product_desc = product_description,
                ounces = product_size, store_zip = store_zip_code)
```

I reordered the columns. 
 
```{r reorder_columns_clean, message = FALSE, warning = FALSE, error = FALSE}
# Reorder columns
carbo <- carbo %>%
  select(household, week, feature_desc, display_desc, commodity, brand, 
         product_attribute, ounces, product_size_factor, upc, product_desc, 
         coupon, quantity, dollar_sales, day, time, basket, everything())
```


### 3.5 Summary of Prepared Data

In the cleaned dataset, each observation is a household's UPC from a specific 
day. This prepared dataset contains `r comma(dim(carbo)[1])` observations and 
`r dim(carbo)[2]` variables.

**The key variables I will be looking at are:**

* **commodity** - which has 4 values: `r unique(sort(carbo$commodity))`

* **product_desc** - which is the name of the products. There are 
`r length(unique(carbo$product_desc))` products.

* **product_attribute** - which is a keyword describing the product. There are 
`r length(unique(carbo$product_attribute))` attributes.

* **feature_desc** - 8 ordered levels describing weekly mailer: 
`r levels(carbo$feature_desc)`

* **display_desc** - 11 ordered levels describing in-store display: 
`r levels(carbo$display_desc)`

```{r glimpse_clean, echo=FALSE, message = FALSE, warning = FALSE}
glimpse(carbo)
```

**The first 500 observations**

```{r, table_clean, echo=FALSE, message = FALSE, warning = FALSE, error = FALSE}
# First 500 rows of the cleaned data
datatable(head(carbo, 500))
```


## 4 Exploratory Data Analysis {.tabset .tabset-fade}


```{r carbo_backup, include = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Used in development so I don't have to keep recreating the dataset from the start
carbo_backup <- carbo
```

```{r carbo_rebackup, include = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Used in in development so I don't have to keep recreating the dataset from the start
carbo <- carbo_backup
```




### 4.1 Promotion Types And Sales

First I looked at the frequency of promotion types and resulting sales by 
commodity and attribute. 

**Promotion Type Pairings**

I looked at the paired promotions (meaning promoted in both mailer and in-store 
displays). A blank cell indicates that commodity has never been promoted in that 
combination. For example, pastas that are displayed in the seasonal aisle have 
never been part of the weekly mailer wrap. Pancakes have appeared on the front 
of the weekly mailer wrap, as intertior page and back page features, but 
otherwise are absent from weekly mailers. Similarly, syrups are mostly absent 
from weekly mailers but when they do appear in the mailer it is never on the 
front, but only on the back of the wrap.  I see that pasta and sauces are more 
commonly promoted across all categories of in-store displays and weekly mailer.


```{r most_frequent_promoted, message = FALSE, warning = FALSE, error = FALSE}
# Promoted product combinations
carbo %>%
  ggplot(aes(display_desc, fct_rev(feature_desc), color = commodity)) +
  geom_point() +
  theme_bw() +
  scale_color_brewer(palette = 'Set1') +
  scale_x_discrete(name = 'In-Store Display Promotion') +
  scale_y_discrete(name = 'Weekly Mailer Promotion') +
  ggtitle('Pasta and Sauces Dominate Promotion Types') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(~ commodity)
```

**Promotion Sales Frequency**

I now look at how frequently the promoted items resulted in a sale, measured by 
the number of shopping baskets.  

```{r function_frequency_promo, message = FALSE, warning = FALSE, error = FALSE}
# Function: promotion frequency
plot_promotion_frequency <- function(column, axis_label) {
  col_name <- as.name(column)
  carbo %>%
    ggplot(aes(fct_rev(fct_infreq(!! col_name)), fill = commodity)) +
    geom_bar() +
    theme_bw() +
    scale_fill_brewer(palette = 'Set1') +
    scale_x_discrete(name = axis_label) +
    scale_y_continuous(name = 'Number of Baskets', labels = comma) +
    ggtitle('Distribution of Shopping Baskets', subtitle = axis_label) +
    theme(legend.position = 'none') +
    facet_wrap(~ commodity) +
    coord_flip()
}
```


First I looked at in-store display promotions that results in sales. 

Most of the pastas and sauces sold were not displayed in an in-store promotion. 
Those that were promoted were typically in rear or front endcaps or in-shelf 
promotions. The least frequent sales were side-aisle endcaps and store rear/front. 
It would be worth evaluating if that is because those promotions are more rare, 
or just less successful. 


Pancakes/syrups overall appear in less baskets than the pastas/sauces. However, 
in-shelf promotions and secondary location displays are proportionally more 
successful for pancakes/syrups than for pastas/sauces. 


```{r call_function_display, message = FALSE, warning = FALSE, error = FALSE}
# Call function for display desc
plot_promotion_frequency(column = 'display_desc', 
                         axis_label = 'In-Store Display Promotion')
```

Next I looked at weekly mailer promtions that resulted in sales.

For pastas/sauces, the interior page feature resulted in the most sales, 
which is not surprising since that is where most products are promoted. For 
sauces, a front page feature seems be a significant boost above other categories. 
Surprisingly, wraps are more successful if featured in the interior rather 
than the exterior. 

While pancakes/syrups are featured less, it is surprising that the most items 
sold were not featured. Pancakes are most successful as a back page feature or 
front wrap. Syrups are most successful as an interior page feature. 

```{r call_function_feature, message = FALSE, warning = FALSE, error = FALSE}
# Call function for feature frequency
plot_promotion_frequency(column = 'feature_desc', axis_label = 'Weekly Mailer Promotion')
```

**Promotion Attributes**


I now look at the variable "product attribute". I created this variable from 
product descriptions in order to compare like products. I now looked at the 15 
most frequently identified product attributes, measured by the number of baskets. 


Attributes describing the sauces are the most frequent. Pastas are the second 
most frequent. Pancakes only have one attribute that is frequent (maple) and 
syrup only has two (maple and lite).   

```{r most_freq_product_attribute, message = FALSE, warning = FALSE, error = FALSE}
# Most frequent sold products identified by  attributes
carbo %>%
  mutate(new_attribute = fct_lump(product_attribute, n = 15, 
                                  other_level = 'all other attributes')) %>%
  ggplot(aes(fct_rev(fct_infreq(new_attribute)), fill = commodity)) +
  geom_bar() +
  theme_bw() +
  scale_fill_brewer(palette = 'Set1') +
  scale_x_discrete(name = 'Product Attribute') +
  scale_y_continuous(name = 'Number of Baskets', labels = comma) +
  ggtitle('Most Frequently Sold Product Attributes') +
  theme(legend.position = 'none') +
  facet_wrap(~ commodity) +
  coord_flip()
```


**Promotion Commodity: Sauce**

Because it encompasses the most mentioned attributes, I dig further into the 
Sauce commodity. Vegetable and mushroom sauces have the most sales.  

```{r function_plot_attribute_freq, message = FALSE, warning = FALSE, error = FALSE}
# Most frequent sold products identified by  attributes

plot_attribute_frequency <- function(column, legend_text) {
  col_name <- as.name(column)
  carbo %>%
  filter(commodity == 'SAUCE') %>%
  ggplot(aes(fct_rev(fct_infreq(product_attribute)), fill = !! col_name)) +
  geom_bar() +
  theme_bw() +
  labs(fill = legend_text) +
  scale_fill_brewer(palette = 'Set3') +
  scale_x_discrete(name = 'Product Attribute') +
  scale_y_continuous(name = 'Number of Baskets', labels = comma) +
  ggtitle('Product Attribute Distribution for Sauces', subtitle = legend_text) +
    coord_flip() 
}
```


By coloring the chart by weekly mailer promotions, I also see that sauce products 
which are promoted in-store represent a large chunk of the sales. I also see rear 
end cap is significant. 

```{r call_attribute_display, message = FALSE, warning = FALSE, error = FALSE}
# Call function to plot attributes for display_desc
plot_attribute_frequency(column = 'display_desc', 
                         legend_text = 'In-Store Display Promotion')
```
By coloring the chart by in-store display promotions, I see that sauce products 
promoted in the interior and front page of the mailers have the largest influence. 
These results seem to hold for all attributes.

```{r call_att_feature, message = FALSE, warning = FALSE, error = FALSE}
# Call function to plot attributes for feature_desc
plot_attribute_frequency(column = 'feature_desc', 
                         legend_text = 'Weekly Mailer Promotion')
```


### 4.2 Clustered Products

When we talk about market segmentation, we often think of grouping people so 
that we can engage with them in similar ways within a cluster. But as mentioned, 
the sampling used for the data does not support a longitudal view of customers 
as many customers only have one basket over the two year period. Also since the 
products are limited to a few commodities, it is not possible to identify the 
interactions with other commodity promotions or sales. So instead I segmented 
the data by products. I used a hierarchical clustering model and then analyzed 
what differentiates the products in the resulting clusters.

**Aggregating to Product Grain**

Exploratory Data Analysis involves slicing the data to view it from different 
angles. The initial dataframe was organized as each row represents one UPC per 
basket per household. I rearranged the data so that one row represents one 
product over the entire period. I did this by summarizing the quantity of sales, 
number baskets, number weeks, number stores, by each product (upc). This is 
referred to as "aligning the grain" and I aligned the grain to product.

```{r create_carbo_product, message = FALSE, warning = FALSE, error = FALSE}
# Creating new dataframe at product grain with each row a different upc
carbo_product = carbo %>%
                group_by(upc, commodity, brand, product_attribute, 
                         product_size_factor, product_desc) %>%
                summarize(num_households = n_distinct(household), 
                  percent_coupons = mean(coupon), 
                  total_quantity = sum(quantity), 
                  avg_quantity = mean(quantity),
                  max_quantity = max(quantity),
                  num_baskets = n_distinct(basket),
                  num_weeks = n_distinct(week),
                  first_week = min(week),
                  last_week = max(week),
                  avg_dollars = round(mean(dollar_sales),2),
                  total_dollars = round(sum(dollar_sales),2),
                  num_stores = n_distinct(store),
                  num_regions = n_distinct(geography)
                  )
```

I counted the number of times a product was in each level of the weekly mailers 
and in-store displays. 

```{r count_feature_and_display, message = FALSE, warning = FALSE, error = FALSE}
# Count feature descriptions
level_names <- carbo %>%
    pull(feature_desc) %>%
    levels() 

level_names_underscore <- str_replace_all(level_names,' ','_') #replace spaces
level_names_underscore <- str_remove(level_names_underscore, '_Feature') #remove duplicate word 

for (i in seq_along(level_names_underscore)) {
  variable_name_new <- paste0('num_feature_', level_names_underscore[i])
  carbo_product1 <- carbo %>% group_by(upc) %>%
                summarize(!!variable_name_new := sum(feature_desc == level_names[i]))
  
  carbo_product <- carbo_product %>%
          left_join(carbo_product1, by = 'upc')
}

# Count display descriptions
level_names <- carbo %>%
    pull(display_desc) %>%
    levels() 

level_names_underscore <- str_replace_all(level_names,' ','_') #replace spaces
level_names_underscore <- str_remove(level_names_underscore, '_Display') #remove duplicate word

for (i in seq_along(level_names_underscore)) {
  variable_name_new <- paste0('num_display_', level_names_underscore[i])
  carbo_product1 <- carbo %>% group_by(upc) %>%
                summarize(!!variable_name_new := sum(display_desc == level_names[i]))
  
  carbo_product <- carbo_product %>%
          left_join(carbo_product1, by = 'upc')
}

# Ungroup and clean environment
carbo_product <- carbo_product %>% ungroup()
rm(carbo_product1)
```


I convert all characters and factors to numeric for modeling. This numeric 
dataframe is called carbo_product_numbers but I also keep the original carbo_product 
dataframe in order to preserve the categorical names which I will later use. 

```{r convert_product_to_numeric, message = FALSE, warning = FALSE, error = FALSE}
# Convert to numbers for modeling
carbo_product_numbers <- carbo_product %>% 
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.factor, as.numeric)
```

This new dataset, aligned to the product grain, contains `r dim(carbo_product)[1]` 
observations and `r dim(carbo_product)[2]` variables.

**Modeling With Hierarchical Clustering**

I then analyzed this newly created product-grain dataframe, by grouping like 
products together using a clustering algorithm. Clustering is a type of "unsupervised" 
learning in that we do not have a specific target we are predicting (such as sales 
volume). Instead the model is fit by measuring "distances" between variables. 
Using math, the computer divides observations into meaningful groups that share 
common characteristics. 
These cluster members are similar to one another, and distinctly different from 
others outside the cluster. 

The inputs to the model are the variables of interest in numeric format, a way 
to measure distance, and the number of clusters desired. 

For the distance, I picked jiccard distance which is used for categorical data. 
I calculated the distance between each point using the function dist() with method 
"binary". Then I create a linkage with the hierarchical cluster function hclust() 
and method "complete" (which calculates maximum distance). 

```{r calculate_distance, message = FALSE, warning = FALSE, error = FALSE}
# Calculate distance
dist_carbo <- dist(carbo_product_numbers, method = 'binary') #jiccard distance
hc_carbo <- hclust(dist_carbo, method = 'complete') #linkage
```


There are many ways we can use to decide the optimum number of clusters (k). 
We can use domain expertise to just pick a number for k, or we can assign 
observations to a cluster and then calculate distances using silhouette width 
or plot the distances using dendrograms. 


The silhouette width calculates how similar (close in distance) each observation 
is with the cluster it is assigned, relative to other clusters. The plots shows 
the average silhouette width of all observations within a cluster, at different 
k values. A silhouette width of 1 indicates the observation is well matched to 
the current cluster, 0 indicates it is in-between clusters, and -1 indicates it 
should be assigned to a different cluster.  We want the average silhouette width 
to be as close to 1 as possible. With our data, using 2 clusters has an average 
silhouette width of almost .8 which is really good. In contrast, if we use 8 
clusters, our average silhouette width is close to 0 which means it is a poor assignment. 

```{r best_num_clusters, message = FALSE, warning = FALSE, error = FALSE}
# Silhouette width comparing K
sil_width <- map_dbl(2:10, function(k){
  model <- pam(x = carbo_product_numbers, k = k)
  model$silinfo$avg.width
})
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

ggplot(sil_df, aes(x = k, y = sil_width)) +
geom_line() +
theme_bw() +
labs(y = 'Silhouette Width', x = 'Number of Clusters (K)') +
ggtitle('Silhouette Width Identifies 2 Clusters As Optimal') +
scale_x_continuous(breaks = 2:10)
```



We can also visualze the optimal number of clusters with a dendrogram, where the 
height of each branch is determined by linkage and distance decisions we made 
during clustering. The x-axis plots observations. Because there are so many 
observations, the labels are hard to read but the guide on the bottom shows the 
distribution of the 400 observations. I plotted 2, 3, and 7 clusters. By 
comparing, we can see that adding more clusters results in some very narrow clusters.

```{r plot_color_dendrogram, message = FALSE, warning = FALSE, error = FALSE}
# Color dendrogram to show cluster height
dend_clustered <- as.dendrogram(hc_carbo) #converts hclust to dendrogram
dend_colored <- color_branches(dend_clustered, k = 2)
p1 <- ggplot(dend_colored) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme_classic() +
  scale_fill_brewer(palette = 'Set1') +
  scale_y_continuous(name = 'Height') +
  ggtitle('Dendrogram 2 Clusters')

# Color dendrogram to show cluster height
dend_clustered <- as.dendrogram(hc_carbo) #converts hclust to dendrogram
dend_colored <- color_branches(dend_clustered, k = 3)
p2 <- ggplot(dend_colored) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme_classic() +
  scale_fill_brewer(palette = 'Set1') +
  scale_y_continuous(name = 'Height') +
  ggtitle('Dendrogram 3 Clusters')

# Color dendrogram to show cluster height
dend_clustered <- as.dendrogram(hc_carbo) #converts hclust to dendrogram
dend_colored <- color_branches(dend_clustered, k = 7)
p3 <- ggplot(dend_colored) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme_classic() +
  scale_fill_brewer(palette = 'Set1') +
  scale_y_continuous(name = 'Height') +
  ggtitle('Dendrogram 7 Clusters')

grid.arrange(p1, p2, p3, ncol = 3)
```

**Analyze the Model Clusters**


Despite both methods suggesting 2 clusters, I decide to use 3 clusters. My reason 
is that I would like contrast the product clusters and there would be more 
meaningful information comparing 3 groups. 

```{r fit_cluster_model, message = FALSE, warning = FALSE, error = FALSE}
# Fitting model and append cluster number
cluster_assignment <- cutree(hc_carbo, k = 3) #extract cluster assignment
carbo_product$cluster = cluster_assignment #append assignment to original dataframe
```

It's hard to visualize clusters with more than two variables. But we can use 
summary statistics to build a narrative of what makes the observations similar 
to each other within the cluster while different from observations in other 
clusters. Each row is a different cluster and the columns are summary statistics 
for that cluster. 

```{r cluster_stats, message = FALSE, warning = FALSE, error = FALSE}
# Statistics on clustered
clustered_stats <- carbo_product %>% 
  select(-(1:6)) %>%
  group_by(cluster) %>% 
  summarise_all(funs(round(mean(.),2)))

datatable(clustered_stats)
```

Cluster 1 is small, but differentiates from the other clusters in that it appears 
to be the least influenced by the weekly mailers. In fact there were 0 sales 
in some weekly mailer feature categories (wrap front, back page, and interior 
line item). They also were purchased in the fewest stores for the fewest number 
of weeks. This is not a group that attracts store visits from weekly promotions.

Cluster 2 is the largest cluster and also appears to be the most influenced to 
purchase items in the weekly mailers. These products are bought in higher 
quantities and for the longest period (stretching from week 47 to week 100). 
Whereas the other two clusters averaged 0 and 1.1 sales resulting from wrap 
front, this group averaged 70 from the wrap front and 37 from the wrap back. 
It would be worth looking further into what motivates this group in order to 
focus our promotion campaigns. 

Cluster 3 is between the other two in terms of influence by mailers and displays, 
but they purchase a higher percent of the time with coupons. It's interesting that 
they aren't as influenced as cluster 2 by the mailers because responding to 
mailers and collecting coupons both reflect coming to the store with an incentive 
to purchase. Also interesting is that they also do not seem influencd by store 
displays. It could be people buying this product come to the store focues on 
what they came to purchase and not much extra.

Even though these statistics reflect sums and aggregations, when the model was 
fit it considered the influence of commodity, brand, product attribute, product 
size. We can look at specific products assigned to each for insight.

Here's the entire Product-Grain table showing the cluster assignments.


```{r table_cluster_assig, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Product-grain dataframe with cluster assignment
datatable(carbo_product %>%
  select(cluster, everything()) %>%
  arrange(cluster) )
```



We can also look the category distributions within clusters. I looked at how 
brand was included in each cluster. I singled out the three lines of Private 
Selection products and lumped all other brands together. 

 

```{r table_kroger,  message = FALSE, warning = FALSE, error = FALSE}
carbo_product %>% 
  mutate(kroger_brand = fct_other(brand, 
          keep = c('PRIVATE LABEL VALUE', 'PRIVATE LABEL', 'PRIVATE LABEL PREMIUM'))) %>% 
  group_by(cluster, commodity) %>% 
  count(kroger_brand) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(kroger_brand, n, color = commodity)) + 
  geom_point() +
  facet_grid(commodity ~ as.factor(cluster)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_color_brewer(palette = 'Set1') +
  ggtitle('Cluster Distribution of Kroger Brands') +
  labs(x = 'Brand', y = 'Number Products') +
  coord_flip()
```

Through this lens, we see that cluster 1 contains the most syrup products and 
the most pasta products. The most popular of the Kroger lines in this cluster 
is the premium label. Previously we saw this was also the smallest cluster. 


Cluster 2 was the largest cluster. It also includes the highest number of 
Private Label brand products, plus two premium and 1 value category. There is 
not as much of a difference between the number of Kroger products and the other 
products within this cluster, suggeting people who purchase these products may 
be loyal Kroger shoppers.   


Cluster 3 products emphasize the sauce commodity. Cluster 3 also has the fewest 
Kroger brand products. Even though this was the cluster that was also coupon 
heavy, there were no purchases in the Kroger value label. This sheds light that 
the coupons redeemed are most likely manufacturer coupons and not the result 
of precision coupon mailings.  



```{r clean_product_environment, message = FALSE, warning = FALSE, error = FALSE}
# Clean environment
remove_objects <- c('cluster_assignment', 'carbo_product_numbered', 'hc_carbo', 
                    'dend_colored', 'dend_clustered', 'clustered_stats', 'sil_df')
rm(list = ls()[ls() %in% remove_objects])
```


## 5 Summary


### Business Question Addressed
My goal was to analyze how weekly mailers and in-store displays connect with 
sales for carbohydrate products. I looked at promotion types and resulting sales 
plus I segmented products into clusters and analyzed what differentiated the 
clusters. 

### Insights and Implications 

**Connection between promotions types and sales**

Regarding pastas and sauces, I was surprised to see that being featured on the 
wrap of weekly mailers does little to drive sales. In fact the most effective 
promotions were on rear end caps for in-store promotions and as interior features 
in weekly mailers. Both of these were surprising as I thought more prominence 
(front page, front of store, outside of mailer wrap) would result in more sales. 
It was also surprising to see the how much more common regular spaghetti and thin 
spaghetti were over other pastas and how vegetable and mushroom sauces dominated 
the sales. 

I also found that the current promotion of pancake mixes and syrups did not 
result in many sales. These promotions were most effective on the back or 
interior pages of weekly mailers and secondary locations within the store. 
The most common attribute was maple for the mixes and maple/lite for the syrups. 

I see two implications. First, I would continue promoting pastas and sauces in 
less prominent locations of the mailers and in the rear end caps of the stores. 
These have been effective in generating sales, and leaves premium space to promote 
other products in commodities where you may want to boost sales or entice 
customers into the store. Second, I would promote a diversified product line 
for pancakes and syrups. The current promotions are driving "maple" sales, but 
those sales are low. I suggest a strategy to use the wrap or back page (where 
these commodities have been most effective) to promote unique products (such as 
boysenberry syrups or buckwheat pancakes) to potentially boost sales by generating 
interest in substitute products. I would place these products in secondary 
locations of the store where customers are currently looking for pancakes and 
syrups, but draw attention to them. 

**Differentiating products with clustering**

By clustering the products, we identified three distinct groups of products. An 
interesting insight was how the model defined the groups. A very large group of 
products was created that were bought in higher quantities and responded to by 
promotions. But one cluster appears to not be influences by promotions. Another 
cluster appears to have products that have not been part of display or mailer 
promotions but are used with coupons. We also looked at this through the lens 
of Kroger brands and saw that the group that was most coupon focused, bought the 
least Kroger products and didn't buy any in the Kroger Value Line.

An implication is to look further into the clusters for possibilities for cross 
marketing complimentary products.  For example, cluster 1 is highest in syrup, 
so we could look at complimentary pancake mixes that could be displayed in similar 
locations. Cluster 3 is highest in coupons, but not in Kroger products. We 
could market Kroger products with coupons that are complimentary to the sauces 
they are purchasing. One idea could be recipes using the sauces but including 
other Kroger products such as spices or vegetables and include coupons for those 
other products to go with the sauces they are already purchasing. 

### Limitations

A limitation of the study is the limited variety of promotion campaigns. An 
extension of this analysis could include the impact of additional ways of 
engaging with customers, such as through other Kroger marketing channels and 
digital ads across the web. Another extension could look at the specifics of 
coupons redeemed as to whether they were a result of customer specific mailers, 
general vendor coupons (such as from the Sunday paper), or digital coupons 
from the Kroger app.

Another limitation of the study is the stratificatoin of households in the sample. 
The data could be redrawn in a way to better reflect the purchaing of a consumer 
over the entire time period so we could classify specific consumer sales uplift 
as a result of promotions. The current data does not have enough baskets per 
consumer to justify this type of analysis. 
